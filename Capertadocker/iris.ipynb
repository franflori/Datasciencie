{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29af355a-f8e9-49e9-8b8a-c5b33ee2896f",
   "metadata": {},
   "source": [
    "## MLlib usando Titanic dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "593ff85e-93e5-4ec9-b297-26c057e3d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark .sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"iris\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bac5b9e-eed4-42e1-88f8-9944396f6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.csv(\"iris.csv\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9900c15c-3294-4790-bceb-29f5f687058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+------+\n",
      "|_c0|_c1|_c2|_c3|   _c4|\n",
      "+---+---+---+---+------+\n",
      "|5.1|3.5|1.4|0.2|setosa|\n",
      "|4.9|3.0|1.4|0.2|setosa|\n",
      "|4.7|3.2|1.3|0.2|setosa|\n",
      "|4.6|3.1|1.5|0.2|setosa|\n",
      "|5.0|3.6|1.4|0.2|setosa|\n",
      "|5.4|3.9|1.7|0.4|setosa|\n",
      "|4.6|3.4|1.4|0.3|setosa|\n",
      "|5.0|3.4|1.5|0.2|setosa|\n",
      "|4.4|2.9|1.4|0.2|setosa|\n",
      "|4.9|3.1|1.5|0.1|setosa|\n",
      "+---+---+---+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92e928de-f790-4947-863b-248e2b0be417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>5.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     _c0  _c1  _c2  _c3        _c4\n",
       "0    5.1  3.5  1.4  0.2     setosa\n",
       "1    4.9  3.0  1.4  0.2     setosa\n",
       "2    4.7  3.2  1.3  0.2     setosa\n",
       "3    4.6  3.1  1.5  0.2     setosa\n",
       "4    5.0  3.6  1.4  0.2     setosa\n",
       "..   ...  ...  ...  ...        ...\n",
       "146  6.3  2.5  5.0  1.9  virginica\n",
       "147  6.5  3.0  5.2  2.0  virginica\n",
       "148  6.2  3.4  5.4  2.3  virginica\n",
       "149  5.9  3.0  5.1  1.8  virginica\n",
       "150  5.3  4.3  2.1  0.5     setosa\n",
       "\n",
       "[151 rows x 5 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb10c7f2-d5d9-41fd-8704-3fd813b25143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f54abddb-8355-4d8b-a41a-746f34bbea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3', '_c4']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3720032e-599d-4b1d-8390-408a18b3a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bf32af3-1647-4a49-8311-97ca995d1f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.83973509933775</td>\n",
       "      <td>3.06225165562914</td>\n",
       "      <td>3.747682119205301</td>\n",
       "      <td>1.1940397350993384</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.8264848835236843</td>\n",
       "      <td>0.44388312603721947</td>\n",
       "      <td>1.7637019565707457</td>\n",
       "      <td>0.7627347103748434</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>7.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 _c0                  _c1                 _c2  \\\n",
       "0   count                 151                  151                 151   \n",
       "1    mean    5.83973509933775     3.06225165562914   3.747682119205301   \n",
       "2  stddev  0.8264848835236843  0.44388312603721947  1.7637019565707457   \n",
       "3     min                 4.3                  2.0                 1.0   \n",
       "4     max                 7.9                  4.4                 6.9   \n",
       "\n",
       "                  _c3        _c4  \n",
       "0                 151        151  \n",
       "1  1.1940397350993384       None  \n",
       "2  0.7627347103748434       None  \n",
       "3                 0.1     setosa  \n",
       "4                 2.5  virginica  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79cec65c-ba90-4d00-93a4-aaf3b4fc8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+------+\n",
      "|_c0|_c1|_c2|_c3|   _c4|\n",
      "+---+---+---+---+------+\n",
      "|5.1|3.5|1.4|0.2|setosa|\n",
      "|4.9|3.0|1.4|0.2|setosa|\n",
      "|4.7|3.2|1.3|0.2|setosa|\n",
      "|4.6|3.1|1.5|0.2|setosa|\n",
      "|5.0|3.6|1.4|0.2|setosa|\n",
      "|5.4|3.9|1.7|0.4|setosa|\n",
      "|4.6|3.4|1.4|0.3|setosa|\n",
      "|5.0|3.4|1.5|0.2|setosa|\n",
      "|4.4|2.9|1.4|0.2|setosa|\n",
      "|4.9|3.1|1.5|0.1|setosa|\n",
      "|5.4|3.7|1.5|0.2|setosa|\n",
      "|4.8|3.4|1.6|0.2|setosa|\n",
      "|4.8|3.0|1.4|0.1|setosa|\n",
      "|4.3|3.0|1.1|0.1|setosa|\n",
      "|5.8|4.0|1.2|0.2|setosa|\n",
      "|5.7|4.4|1.5|0.4|setosa|\n",
      "|5.4|3.9|1.3|0.4|setosa|\n",
      "|5.1|3.5|1.4|0.3|setosa|\n",
      "|5.7|3.8|1.7|0.3|setosa|\n",
      "|5.1|3.8|1.5|0.3|setosa|\n",
      "+---+---+---+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Realizamos la transformación de las columnas de string a numeros:\n",
    "from pyspark.sql.functions import col\n",
    "dataset = df.select(col(\"_c0\").cast(\"float\"),\n",
    " col(\"_c1\").cast(\"float\"),\n",
    " col(\"_c2\").cast(\"float\"),\n",
    " col(\"_c3\").cast(\"float\"),\n",
    " col(\"_c4\"))\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0622d2fd-a6cf-4eb3-ac3d-f4868a67030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, when, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15a18d2e-a836-4cc4-8518-c238677c2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.replace(\"?\", None).dropna(how=\"any\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fef99413-5bb0-41e2-86b6-5cf04cd9f8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'float'),\n",
       " ('_c1', 'float'),\n",
       " ('_c2', 'float'),\n",
       " ('_c3', 'float'),\n",
       " ('_c4', 'string')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01604213-8ccb-4641-9c38-8ddd5e8c3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificación de las columnas en formato string a número.\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "dataset = StringIndexer(inputCol=\"_c4\",\n",
    " outputCol=\"tipo\",\n",
    " handleInvalid=\"keep\")\\\n",
    " .fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91df176f-2a85-4d7e-8964-699f6a90d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+------+----+\n",
      "|_c0|_c1|_c2|_c3|   _c4|tipo|\n",
      "+---+---+---+---+------+----+\n",
      "|5.1|3.5|1.4|0.2|setosa| 0.0|\n",
      "|4.9|3.0|1.4|0.2|setosa| 0.0|\n",
      "|4.7|3.2|1.3|0.2|setosa| 0.0|\n",
      "|4.6|3.1|1.5|0.2|setosa| 0.0|\n",
      "|5.0|3.6|1.4|0.2|setosa| 0.0|\n",
      "|5.4|3.9|1.7|0.4|setosa| 0.0|\n",
      "|4.6|3.4|1.4|0.3|setosa| 0.0|\n",
      "|5.0|3.4|1.5|0.2|setosa| 0.0|\n",
      "|4.4|2.9|1.4|0.2|setosa| 0.0|\n",
      "|4.9|3.1|1.5|0.1|setosa| 0.0|\n",
      "|5.4|3.7|1.5|0.2|setosa| 0.0|\n",
      "|4.8|3.4|1.6|0.2|setosa| 0.0|\n",
      "|4.8|3.0|1.4|0.1|setosa| 0.0|\n",
      "|4.3|3.0|1.1|0.1|setosa| 0.0|\n",
      "|5.8|4.0|1.2|0.2|setosa| 0.0|\n",
      "|5.7|4.4|1.5|0.4|setosa| 0.0|\n",
      "|5.4|3.9|1.3|0.4|setosa| 0.0|\n",
      "|5.1|3.5|1.4|0.3|setosa| 0.0|\n",
      "|5.7|3.8|1.7|0.3|setosa| 0.0|\n",
      "|5.1|3.8|1.5|0.3|setosa| 0.0|\n",
      "+---+---+---+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a1462672-ad1a-411f-abd4-7764a9d427e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.839735094285169</td>\n",
       "      <td>3.0622516594185734</td>\n",
       "      <td>3.7476821072054225</td>\n",
       "      <td>1.194039726592847</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9933774834437086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.8264848676137385</td>\n",
       "      <td>0.44388312922627116</td>\n",
       "      <td>1.7637019516072574</td>\n",
       "      <td>0.7627347005839645</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8205420057638477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>7.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 _c0                  _c1                 _c2  \\\n",
       "0   count                 151                  151                 151   \n",
       "1    mean   5.839735094285169   3.0622516594185734  3.7476821072054225   \n",
       "2  stddev  0.8264848676137385  0.44388312922627116  1.7637019516072574   \n",
       "3     min                 4.3                  2.0                 1.0   \n",
       "4     max                 7.9                  4.4                 6.9   \n",
       "\n",
       "                  _c3        _c4                tipo  \n",
       "0                 151        151                 151  \n",
       "1   1.194039726592847       None  0.9933774834437086  \n",
       "2  0.7627347005839645       None  0.8205420057638477  \n",
       "3                 0.1     setosa                 0.0  \n",
       "4                 2.5  virginica                 2.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "938db974-9848-4c99-9bda-a4cc4ad5aca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+\n",
      "|_c0|_c1|_c2|_c3|tipo|\n",
      "+---+---+---+---+----+\n",
      "|5.1|3.5|1.4|0.2| 0.0|\n",
      "|4.9|3.0|1.4|0.2| 0.0|\n",
      "|4.7|3.2|1.3|0.2| 0.0|\n",
      "|4.6|3.1|1.5|0.2| 0.0|\n",
      "|5.0|3.6|1.4|0.2| 0.0|\n",
      "|5.4|3.9|1.7|0.4| 0.0|\n",
      "|4.6|3.4|1.4|0.3| 0.0|\n",
      "|5.0|3.4|1.5|0.2| 0.0|\n",
      "|4.4|2.9|1.4|0.2| 0.0|\n",
      "|4.9|3.1|1.5|0.1| 0.0|\n",
      "|5.4|3.7|1.5|0.2| 0.0|\n",
      "|4.8|3.4|1.6|0.2| 0.0|\n",
      "|4.8|3.0|1.4|0.1| 0.0|\n",
      "|4.3|3.0|1.1|0.1| 0.0|\n",
      "|5.8|4.0|1.2|0.2| 0.0|\n",
      "|5.7|4.4|1.5|0.4| 0.0|\n",
      "|5.4|3.9|1.3|0.4| 0.0|\n",
      "|5.1|3.5|1.4|0.3| 0.0|\n",
      "|5.7|3.8|1.7|0.3| 0.0|\n",
      "|5.1|3.8|1.5|0.3| 0.0|\n",
      "+---+---+---+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.drop(\"_c4\")\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c706dcf-bc15-4b51-b98c-357904f33837",
   "metadata": {},
   "source": [
    "## Realizamos un vetor con la información correspondiente a la X: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da65e3bb-4248-4d19-97cb-42bd139ab8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_features = ['_c0', '_c1',\n",
    " '_c2', '_c3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "51b0f2e5-2864-44e4-93f4-0047c6580da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e91cb4fa-cd64-4ac5-ba03-bcfdfdb2fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=required_features,\n",
    " outputCol=\"feature\")\n",
    "transformed_data = assembler.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "875ccd76-a936-4738-934c-49a18dda8c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+--------------------+\n",
      "|_c0|_c1|_c2|_c3|tipo|             feature|\n",
      "+---+---+---+---+----+--------------------+\n",
      "|5.1|3.5|1.4|0.2| 0.0|[5.09999990463256...|\n",
      "|4.9|3.0|1.4|0.2| 0.0|[4.90000009536743...|\n",
      "|4.7|3.2|1.3|0.2| 0.0|[4.69999980926513...|\n",
      "|4.6|3.1|1.5|0.2| 0.0|[4.59999990463256...|\n",
      "|5.0|3.6|1.4|0.2| 0.0|[5.0,3.5999999046...|\n",
      "|5.4|3.9|1.7|0.4| 0.0|[5.40000009536743...|\n",
      "|4.6|3.4|1.4|0.3| 0.0|[4.59999990463256...|\n",
      "|5.0|3.4|1.5|0.2| 0.0|[5.0,3.4000000953...|\n",
      "|4.4|2.9|1.4|0.2| 0.0|[4.40000009536743...|\n",
      "|4.9|3.1|1.5|0.1| 0.0|[4.90000009536743...|\n",
      "|5.4|3.7|1.5|0.2| 0.0|[5.40000009536743...|\n",
      "|4.8|3.4|1.6|0.2| 0.0|[4.80000019073486...|\n",
      "|4.8|3.0|1.4|0.1| 0.0|[4.80000019073486...|\n",
      "|4.3|3.0|1.1|0.1| 0.0|[4.30000019073486...|\n",
      "|5.8|4.0|1.2|0.2| 0.0|[5.80000019073486...|\n",
      "|5.7|4.4|1.5|0.4| 0.0|[5.69999980926513...|\n",
      "|5.4|3.9|1.3|0.4| 0.0|[5.40000009536743...|\n",
      "|5.1|3.5|1.4|0.3| 0.0|[5.09999990463256...|\n",
      "|5.7|3.8|1.7|0.3| 0.0|[5.69999980926513...|\n",
      "|5.1|3.8|1.5|0.3| 0.0|[5.09999990463256...|\n",
      "|5.4|3.4|1.7|0.2| 0.0|[5.40000009536743...|\n",
      "|5.1|3.7|1.5|0.4| 0.0|[5.09999990463256...|\n",
      "|4.6|3.6|1.0|0.2| 0.0|[4.59999990463256...|\n",
      "|5.1|3.3|1.7|0.5| 0.0|[5.09999990463256...|\n",
      "|4.8|3.4|1.9|0.2| 0.0|[4.80000019073486...|\n",
      "|5.0|3.0|1.6|0.2| 0.0|[5.0,3.0,1.600000...|\n",
      "|5.0|3.4|1.6|0.4| 0.0|[5.0,3.4000000953...|\n",
      "|5.2|3.5|1.5|0.2| 0.0|[5.19999980926513...|\n",
      "|5.2|3.4|1.4|0.2| 0.0|[5.19999980926513...|\n",
      "|4.7|3.2|1.6|0.2| 0.0|[4.69999980926513...|\n",
      "|4.8|3.1|1.6|0.2| 0.0|[4.80000019073486...|\n",
      "|5.4|3.4|1.5|0.4| 0.0|[5.40000009536743...|\n",
      "|5.2|4.1|1.5|0.1| 0.0|[5.19999980926513...|\n",
      "|5.5|4.2|1.4|0.2| 0.0|[5.5,4.1999998092...|\n",
      "|4.9|3.1|1.5|0.1| 0.0|[4.90000009536743...|\n",
      "|5.0|3.2|1.2|0.2| 0.0|[5.0,3.2000000476...|\n",
      "|5.5|3.5|1.3|0.2| 0.0|[5.5,3.5,1.299999...|\n",
      "|4.9|3.1|1.5|0.1| 0.0|[4.90000009536743...|\n",
      "|4.4|3.0|1.3|0.2| 0.0|[4.40000009536743...|\n",
      "|5.1|3.4|1.5|0.2| 0.0|[5.09999990463256...|\n",
      "|5.0|3.5|1.3|0.3| 0.0|[5.0,3.5,1.299999...|\n",
      "|4.5|2.3|1.3|0.3| 0.0|[4.5,2.2999999523...|\n",
      "|4.4|3.2|1.3|0.2| 0.0|[4.40000009536743...|\n",
      "|5.0|3.5|1.6|0.6| 0.0|[5.0,3.5,1.600000...|\n",
      "|5.1|3.8|1.9|0.4| 0.0|[5.09999990463256...|\n",
      "|4.8|3.0|1.4|0.3| 0.0|[4.80000019073486...|\n",
      "|5.1|3.8|1.6|0.2| 0.0|[5.09999990463256...|\n",
      "|4.6|3.2|1.4|0.2| 0.0|[4.59999990463256...|\n",
      "|5.3|3.7|1.5|0.2| 0.0|[5.30000019073486...|\n",
      "|5.0|3.3|1.4|0.2| 0.0|[5.0,3.2999999523...|\n",
      "|7.0|3.2|4.7|1.4| 1.0|[7.0,3.2000000476...|\n",
      "|6.4|3.2|4.5|1.5| 1.0|[6.40000009536743...|\n",
      "|6.9|3.1|4.9|1.5| 1.0|[6.90000009536743...|\n",
      "|5.5|2.3|4.0|1.3| 1.0|[5.5,2.2999999523...|\n",
      "|6.5|2.8|4.6|1.5| 1.0|[6.5,2.7999999523...|\n",
      "|5.7|2.8|4.5|1.3| 1.0|[5.69999980926513...|\n",
      "|6.3|3.3|4.7|1.6| 1.0|[6.30000019073486...|\n",
      "|4.9|2.4|3.3|1.0| 1.0|[4.90000009536743...|\n",
      "|6.6|2.9|4.6|1.3| 1.0|[6.59999990463256...|\n",
      "|5.2|2.7|3.9|1.4| 1.0|[5.19999980926513...|\n",
      "|5.0|2.0|3.5|1.0| 1.0|   [5.0,2.0,3.5,1.0]|\n",
      "|5.9|3.0|4.2|1.5| 1.0|[5.90000009536743...|\n",
      "|6.0|2.2|4.0|1.0| 1.0|[6.0,2.2000000476...|\n",
      "|6.1|2.9|4.7|1.4| 1.0|[6.09999990463256...|\n",
      "|5.6|2.9|3.6|1.3| 1.0|[5.59999990463256...|\n",
      "|6.7|3.1|4.4|1.4| 1.0|[6.69999980926513...|\n",
      "|5.6|3.0|4.5|1.5| 1.0|[5.59999990463256...|\n",
      "|5.8|2.7|4.1|1.0| 1.0|[5.80000019073486...|\n",
      "|6.2|2.2|4.5|1.5| 1.0|[6.19999980926513...|\n",
      "|5.6|2.5|3.9|1.1| 1.0|[5.59999990463256...|\n",
      "|5.9|3.2|4.8|1.8| 1.0|[5.90000009536743...|\n",
      "|6.1|2.8|4.0|1.3| 1.0|[6.09999990463256...|\n",
      "|6.3|2.5|4.9|1.5| 1.0|[6.30000019073486...|\n",
      "|6.1|2.8|4.7|1.2| 1.0|[6.09999990463256...|\n",
      "|6.4|2.9|4.3|1.3| 1.0|[6.40000009536743...|\n",
      "|6.6|3.0|4.4|1.4| 1.0|[6.59999990463256...|\n",
      "|6.8|2.8|4.8|1.4| 1.0|[6.80000019073486...|\n",
      "|6.7|3.0|5.0|1.7| 1.0|[6.69999980926513...|\n",
      "|6.0|2.9|4.5|1.5| 1.0|[6.0,2.9000000953...|\n",
      "|5.7|2.6|3.5|1.0| 1.0|[5.69999980926513...|\n",
      "|5.5|2.4|3.8|1.1| 1.0|[5.5,2.4000000953...|\n",
      "|5.5|2.4|3.7|1.0| 1.0|[5.5,2.4000000953...|\n",
      "|5.8|2.7|3.9|1.2| 1.0|[5.80000019073486...|\n",
      "|6.0|2.7|5.1|1.6| 1.0|[6.0,2.7000000476...|\n",
      "|5.4|3.0|4.5|1.5| 1.0|[5.40000009536743...|\n",
      "|6.0|3.4|4.5|1.6| 1.0|[6.0,3.4000000953...|\n",
      "|6.7|3.1|4.7|1.5| 1.0|[6.69999980926513...|\n",
      "|6.3|2.3|4.4|1.3| 1.0|[6.30000019073486...|\n",
      "|5.6|3.0|4.1|1.3| 1.0|[5.59999990463256...|\n",
      "|5.5|2.5|4.0|1.3| 1.0|[5.5,2.5,4.0,1.29...|\n",
      "|5.5|2.6|4.4|1.2| 1.0|[5.5,2.5999999046...|\n",
      "|6.1|3.0|4.6|1.4| 1.0|[6.09999990463256...|\n",
      "|5.8|2.6|4.0|1.2| 1.0|[5.80000019073486...|\n",
      "|5.0|2.3|3.3|1.0| 1.0|[5.0,2.2999999523...|\n",
      "|5.6|2.7|4.2|1.3| 1.0|[5.59999990463256...|\n",
      "|5.7|3.0|4.2|1.2| 1.0|[5.69999980926513...|\n",
      "|5.7|2.9|4.2|1.3| 1.0|[5.69999980926513...|\n",
      "|6.2|2.9|4.3|1.3| 1.0|[6.19999980926513...|\n",
      "|5.1|2.5|3.0|1.1| 1.0|[5.09999990463256...|\n",
      "|5.7|2.8|4.1|1.3| 1.0|[5.69999980926513...|\n",
      "+---+---+---+---+----+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4bf397-b35b-4aa9-bd0a-5bc9251b66df",
   "metadata": {},
   "source": [
    "## Realizamos los modelos... ## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502a36f-c26a-4dbb-bba2-71004f8a421d",
   "metadata": {},
   "source": [
    "**Algoritmo de Decission Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2460bb7-c4bf-4356-b6d6-0ffd98004711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearemos los dataset de entrenamiento y test:\n",
    "training_data, test_data = transformed_data.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2f5918bb-b257-4b89-b8c1-d498a7ed5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"tipo\",\n",
    " featuresCol=\"feature\",\n",
    " maxDepth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d08c5123-8081-4017-9da6-1a0dfc28cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dt.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a7a7d392-03a3-4d18-8d50-ed6f1a1a6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02429880-95a2-4478-b2cb-5ccf16522897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+--------------------+------------------+-----------------+----------+\n",
      "|_c0|_c1|_c2|_c3|tipo|             feature|     rawPrediction|      probability|prediction|\n",
      "+---+---+---+---+----+--------------------+------------------+-----------------+----------+\n",
      "|4.4|3.0|1.3|0.2| 0.0|[4.40000009536743...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.6|3.2|1.4|0.2| 0.0|[4.59999990463256...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.7|3.2|1.3|0.2| 0.0|[4.69999980926513...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.8|3.0|1.4|0.1| 0.0|[4.80000019073486...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.8|3.0|1.4|0.3| 0.0|[4.80000019073486...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.8|3.1|1.6|0.2| 0.0|[4.80000019073486...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.8|3.4|1.9|0.2| 0.0|[4.80000019073486...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.9|2.5|4.5|1.7| 2.0|[4.90000009536743...|[0.0,29.0,0.0,0.0]|[0.0,1.0,0.0,0.0]|       1.0|\n",
      "|4.9|3.1|1.5|0.1| 0.0|[4.90000009536743...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.9|3.1|1.5|0.1| 0.0|[4.90000009536743...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|4.9|3.1|1.5|0.1| 0.0|[4.90000009536743...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|5.0|2.0|3.5|1.0| 1.0|   [5.0,2.0,3.5,1.0]|[0.0,29.0,0.0,0.0]|[0.0,1.0,0.0,0.0]|       1.0|\n",
      "|5.0|2.3|3.3|1.0| 1.0|[5.0,2.2999999523...|[0.0,29.0,0.0,0.0]|[0.0,1.0,0.0,0.0]|       1.0|\n",
      "|5.0|3.0|1.6|0.2| 0.0|[5.0,3.0,1.600000...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|5.0|3.3|1.4|0.2| 0.0|[5.0,3.2999999523...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|5.0|3.5|1.3|0.3| 0.0|[5.0,3.5,1.299999...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|5.1|3.4|1.5|0.2| 0.0|[5.09999990463256...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|5.1|3.5|1.4|0.2| 0.0|[5.09999990463256...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|5.1|3.8|1.5|0.3| 0.0|[5.09999990463256...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|5.1|3.8|1.6|0.2| 0.0|[5.09999990463256...|[28.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "+---+---+---+---+----+--------------------+------------------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d9e8ecd-e78c-4886-a871-176b97c87290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el modelo:\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"tipo\",\n",
    " predictionCol=\"prediction\",\n",
    " metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af49f4a7-fd47-4698-866d-2a4a99b33f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  87.6923076923077\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy: \", accuracy*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33444b52-016b-4d95-94d2-bd61223b4178",
   "metadata": {},
   "source": [
    "**Algortimo de Gradient-boosted tree Classifier**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b35a758-71e5-4471-95d0-49a4016cceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = transformed_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9e60b74-3a06-4455-8849-82f0a7a8c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"tipo\",\n",
    " featuresCol=\"feature\",\n",
    " maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e7bffdbf-27dc-4639-bbb3-2301dc353f8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1079.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 110.0 failed 1 times, most recent failure: Lost task 0.0 in stage 110.0 (TID 100) (61f4a26e2037 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:209)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgbt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1079.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 110.0 failed 1 times, most recent failure: Lost task 0.0 in stage 110.0 (TID 100) (61f4a26e2037 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:209)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "model = gbt.fit(training_data)\n",
    "##Error porque es binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59abec3-80e3-4f94-b86b-aa200d9e291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70950a-2b64-43e4-be12-f556e4039544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  96.29629629629629\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy: \", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4855c3-e36a-4d8e-9311-fce752bc69b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
